# Table Storage Operational Guide üìä

Um guia pr√°tico para operadores monitorarem e otimizarem o consumo de quota do Table Storage durante importa√ß√µes de 4.8M+ dom√≠nios.

## Quick Reference

### Azure Table Storage Limits (por partition key)
```
Throughput:     20,000 RU/s (eventual consistency)
Entity size:    1 MB m√°ximo
Batch size:     100 entities m√°ximo
Request rate:   ~20k requests/s per partition
Max storage:    500 TB per account
```

### NextDnsBetBlocker Import Profile
```
Dataset:        5M+ dom√≠nios (Tranco 4.8M + Hagezi listas)
Batch size:     100 items
Partitions:     32 (hash-based distribution)
Target rate:    18-20k ops/s (m√°ximo seguro)
Expected time:  ~4-5 minutos
Storage:        ~890 MB (1x import)
Cost:           ~$0.50 (transaction cost)
```

---

## High-Throughput Architecture (Internals)

Para atingir a importa√ß√£o massiva com estabilidade, o sistema utiliza um conjunto de padr√µes de resili√™ncia. Esta se√ß√£o serve como refer√™ncia para implementa√ß√£o de sistemas similares de alta vaz√£o.

### 1. Hierarchical Token Bucket com Burst Control
O sistema limita a vaz√£o em dois n√≠veis simult√¢neos usando o algoritmo Token Bucket:
1. **Global Limit**: Protege a largura de banda da rede e CPU do container (ex: 20k ops/s).
2. **Partition Limit**: Protege parti√ß√µes individuais do Azure Table Storage (ex: 2k ops/s - limite f√≠sico da Azure).

**Corre√ß√£o de Burst**: Diferente de implementa√ß√µes ing√™nuas, o *Burst Capacity* (rajada permitida, geralmente 10% do rate) √© recalculado dinamicamente. Se uma parti√ß√£o sofre degrada√ß√£o (ex: cai para 1000 ops/s), o burst √© ajustado proporcionalmente (100 ops), evitando picos que casariam novos erros 429.

### 2. Backpressure (Contrapress√£o) via Bounded Channels
Para evitar *Out of Memory* (OOM) quando a escrita (Storage) √© mais lenta que a leitura (Download/Parsing):
- Utilizamos `System.Threading.Channels` com capacidade limitada (`BoundedChannel`).
- Se o canal enche (ex: 500 batches na fila), o **Produtor (Parser)** √© suspenso (`await WriteAsync`).
- Isso propaga a lentid√£o "para tr√°s" at√© a origem, equilibrando o sistema sem descartar dados.

### 3. Adaptive Circuit Breaker com Step Recovery
Em vez de falhar ou tentar cegamente, o sistema monitora erros `429 Too Many Requests`:
1. **Degrada√ß√£o**: Ao encontrar erro, reduz o limite da parti√ß√£o (ex: -10%).
2. **Circuit Breaker**: Se a redu√ß√£o atingir o piso (ex: 50%), abre o circuito e para de enviar para aquela parti√ß√£o temporariamente.
3. **Step Recovery (Recupera√ß√£o em Degraus)**: A recupera√ß√£o **n√£o √© instant√¢nea**. O sistema sobe o limite em pequenos degraus (ex: +10%) a cada intervalo (ex: 5s) se houver sucesso. Isso evita a oscila√ß√£o ("flapping") entre carga total e erro.

---

## Pre-Import Checklist

### 1. Verificar Quotas Dispon√≠veis
```kusto
// Query Application Insights
customMetrics
| where name == "ImportMetrics.StorageConsumed"
| summarize latest_mb=max(value) by bin(timestamp, 1h)
| tail 1
```

**A√ß√£o**: Se > 400GB acumulado:
- Executar archival job (delete dados > 90 dias)
- Ou provisionar novo storage account

### 2. Verificar Status do Storage Account
```bash
az storage account show \
  --name <storage_account> \
  --resource-group <rg> \
  --query "properties.{created:creationTime, primaryLocation, statusOfPrimary}"
```

**A√ß√£o**: Se status = "unavailable", aguardar ou failover

### 3. Limpar Cache Local (se necess√°rio)
```bash
# Remover checkpoint de falha anterior (for√ßar re-import)
az storage table row delete \
  --account-name <storage> \
  --table-name Checkpoints \
  --partition-key ImportStatus \
  --row-key Tranco_TopDomains
```

---

## Durante o Import

### Monitoramento em Tempo Real

**Terminal 1: Taxa de processamento**
```kusto
customMetrics
| where name == "ImportMetrics.ItemsProcessed"
| summarize count=sum(value) by bin(timestamp, 10s)
| extend ops_per_sec = count / 10
| order by timestamp desc
```

**Terminal 2: Erros e throttles**
```kusto
customMetrics
| where name == "ImportMetrics.Error429Count"
| summarize errors=sum(value) by bin(timestamp, 1m)
| where errors > 0
```

**Terminal 3: Distribui√ß√£o por parti√ß√£o**
```kusto
customMetrics
| where name == "ImportMetrics.PartitionItemCount"
| summarize items=sum(value) by tostring(customDimensions.PartitionKey)
| extend variance_pct = stdev(value) * 100 / avg(value)
```

### Alertas Red Flags

| M√©trica | Normal | Aten√ß√£o | Cr√≠tico |
|---------|--------|---------|---------|
| **Ops/s** | 18-20k | 15-18k | < 10k |
| **Error 429/min** | 0 | 1-5 | > 10 |
| **Partition variance** | < 5% | 5-15% | > 20% |
| **Latency P99** | < 100ms | 100-500ms | > 1000ms |

**A√ß√µes recomendadas:**

```
Se Error 429/min > 5:
  ‚îú‚îÄ Aumentar `MaxRetries` (appsettings: 3 ‚Üí 5)
  ‚îú‚îÄ Reduzir `MaxGlobalOperationsPerSecond` (20k ‚Üí 15k)
  ‚îî‚îÄ Monitor pr√≥ximo ciclo

Se Ops/s < 10k:
  ‚îú‚îÄ Verificar: network latency (ping storage account)
  ‚îú‚îÄ Verificar: CPU/Memory do container
  ‚îî‚îÄ Se persistir: rollback a partition count (32 ‚Üí 16)

Se Partition variance > 20%:
  ‚îú‚îÄ Hash function pode estar ruim
  ‚îú‚îÄ Fazer dump de um partition (10 items)
  ‚îî‚îÄ Verificar: distribui√ß√£o de dom√≠nios
```

---

## Post-Import Validation

### Verificar Completude
```sql
-- SQL Query (ap√≥s import via Data Explorer)
SELECT 
  PartitionKey,
  COUNT(*) as ItemCount
FROM DomainListsTable
WHERE ImportedDate = CAST(GETDATE() AS DATE)
GROUP BY PartitionKey
ORDER BY ItemCount DESC
```

**Esperado**: 32 parti√ß√µes, ~156k items cada, variance < 5%

### Verificar Integridade
```kusto
// Application Insights
customMetrics
| where name == "ImportMetrics.ValidationErrors"
| summarize errors=sum(value) by tostring(customDimensions.ErrorType)

// Alertar se > 0 duplicates ap√≥s validation
```

**A√ß√£o**: Se erros > 0.1%:
- Revisar logs do GenericListImporter
- Verificar: source list integrity
- Rerun import

### Atualizar Checkpoint
```bash
# Via Azure Portal ou CLI
az storage table row merge \
  --account-name <storage> \
  --table-name Checkpoints \
  --partition-key ImportStatus \
  --row-key Tranco_TopDomains \
  --entity timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ) status=completed items=4998500
```

---

## Troubleshooting Comum

### Problema: 429 Too Many Requests

**Causa**: Exceeding 20k RU/s per partition

**Diagn√≥stico**:
```kusto
customMetrics
| where name == "ImportMetrics.Error429Count"
| summarize errors=sum(value), ops=sum(value)*20
| extend inferred_ops_per_sec = ops / 300  // 5 min window
```

**Solu√ß√£o**:
1. **Imediato**: Aumentar retry delay (exponential backoff)
2. **M√©dio prazo**: Reduzir parallelism (InitialDegreeOfParallelism: 25 ‚Üí 20)
3. **Longo prazo**: Premium Table Storage (10k RU/s provisioned)

### Problema: Timeout no Import

**Causa**: Network latency ou Container lento

**Diagn√≥stico**:
```bash
# Testar lat√™ncia de rede
ping blob.core.windows.net  # deve ser < 50ms
```

**Solu√ß√£o**:
1. Aumentar timeout (appsettings: 30s ‚Üí 60s)
2. Reduzir batch size (100 ‚Üí 50)
3. Aumentar container memory (1 GB ‚Üí 2 GB)

### Problema: Partition Hot-Spot

**Causa**: Hash distribution ruim (ex: muitos dom√≠nios com mesmo prefixo).

**Sintoma**: Logs `‚ö†Ô∏è Partition {X} throughput warning...` frequentes em uma √∫nica parti√ß√£o enquanto outras est√£o ociosas.

**Diagn√≥stico**:
```kusto
customMetrics
| where name == "ImportMetrics.PartitionItemCount"
| summarize items=sum(value) by PartitionKey
| extend variance = stdev(items) * 100 / avg(items)
| where variance > 20
```

**Solu√ß√£o**:
1. Aumentar partition count (32 ‚Üí 64).
2. O sistema agora inclui **Burst Control Din√¢mico** que mitiga parcialmente picos curtos, mas hotspots sustentados exigem re-particionamento.

---

## Logs e Monitoramento de Throttling

O sistema emite novos logs espec√≠ficos para sa√∫de de vaz√£o:

- **Aviso (`Warning`)**: `Partition {X} throughput warning: 2200 items/sec (Limit: 2000)`
    - *Significado*: A parti√ß√£o excedeu o limite nominal. O Burst Bucket absorveu o excesso temporariamente.
    - *A√ß√£o*: Se for espor√°dico, ignorar. Se cont√≠nuo, indica configura√ß√£o de `MaxConcurrency` muito agressiva.

- **Informa√ß√£o (`Info`)**: `Partition {X} recovering. Limit restored to 1800 ops/sec`
    - *Significado*: O mecanismo de **Step Recovery** subiu um degrau de performance ap√≥s per√≠odo de estabilidade.
    - *A√ß√£o*: Nenhuma. Indica que o sistema est√° se auto-curando.

- **Erro (`Error`)**: `Import completed with issues | Degradation events...`
    - *Significado*: Resumo final indicando que houve gargalos.
    - *A√ß√£o*: Verificar m√©trica `MaxThroughput` no relat√≥rio. Se estiver muito acima do limite (ex: 3000 em limite de 2000), ajustes de throttling s√£o necess√°rios.

---

## Cost Monitoring

### Consumo Esperado (por import)

```
Storage cost:      ~$0.02  (890 MB a $0.018/GB)
Transaction cost:  ~$0.50  (50M ops a $0.01 per 10k ops)
Total per day:     ~$0.52
Monthly:           ~$15.60
Yearly:            ~$187.20
```

### Como Reduzir Custo

| T√©cnica | Economia | Tradeoff |
|---------|----------|----------|
| **Batch size: 100 ‚Üí 1000** | ~10x | Latency +100%, memory +10x |
| **Partition count: 32 ‚Üí 16** | ~2x (contention) | Risk de hot-spots |
| **Retention: 90d ‚Üí 30d** | ~3x | Menos hist√≥rico |
| **Delete archival daily** | ~1x | Operational complexity |

**Recomendado**: Kombiniert:
- Batch size = 100 (manter seguran√ßa)
- 30-day retention (vs 90-day)
- Daily delete de dados antigos

---

## Optimization Playbook

### Fase 1: Baseline (Primeira execu√ß√£o)
```
Objetivo: Entender caracter√≠sticas reais
‚îú‚îÄ Execute com config padr√£o
‚îú‚îÄ Registre: throughput, errors, latency, cost
‚îî‚îÄ Documente: varia√ß√µes vs expectativa
```

### Fase 2: Tune (Iterativo)
```
Se Ops/s < 15k:
  ‚îú‚îÄ Aumentar InitialDegreeOfParallelism (20 ‚Üí 30)
  ‚îú‚îÄ Aumentar MaxGlobalOperationsPerSecond (20k ‚Üí 25k)
  ‚îî‚îÄ Rerun

Se Error 429 > 5/min:
  ‚îú‚îÄ Reduzir MaxGlobalOperationsPerSecond (20k ‚Üí 15k)
  ‚îú‚îÄ Aumentar MaxRetries (3 ‚Üí 5)
  ‚îî‚îÄ Rerun
```

### Fase 3: Production (Est√°vel)
```
Objetivo: Opera√ß√£o confi√°vel com alertas
‚îú‚îÄ Lock configura√ß√£o ap√≥s 3 successful runs
‚îú‚îÄ Setup alertas (429 errors, variance, latency)
‚îú‚îÄ Schedule: Daily @ 02:00 UTC
‚îî‚îÄ Autom√°tico + manual oversight
```

---

## SLA & Health Metrics

### Target Metrics
```
Throughput:  18-20k ops/s
Latency:     < 100ms (p99)
Success:     > 99.9% (< 5k errors em 5M)
429 errors:  < 10/min (ideal: 0)
Variance:    < 10% (partitions balanceados)
Cost:        < $1/dia (target: ~$0.50)
```

### Dashboarding (Application Insights)

```json
{
  "name": "Import Monitoring",
  "tiles": [
    {
      "title": "Ops/sec",
      "metric": "ImportMetrics.ItemsProcessed",
      "aggregation": "sum",
      "timespan": "10s",
      "threshold": [15000, 20000]
    },
    {
      "title": "Error 429",
      "metric": "ImportMetrics.Error429Count",
      "aggregation": "sum",
      "timespan": "1m",
      "threshold": [0, 10]
    },
    {
      "title": "Partition Variance",
      "metric": "ImportMetrics.PartitionItemCount",
      "aggregation": "stdev",
      "threshold": ["< 5%", "> 20%"]
    }
  ]
}
```

---

## Contato & Escalation

**Observado comportamento an√¥malo?**

1. **Coletar logs**: Export Application Insights logs (√∫ltimas 2 horas)
2. **Documentar**: 
   - Hor√°rio do evento
   - M√©trica afetada
   - Screenshot do dashboard
3. **Verificar**: Consultar Azure status page (status.azure.com)
4. **Reportar**: Abrir issue no GitHub com logs + diagnosis

---

**√öltima atualiza√ß√£o**: 2024-01-15  
**Vers√£o**: 1.0 (para 4.8M Tranco + Hagezi)
