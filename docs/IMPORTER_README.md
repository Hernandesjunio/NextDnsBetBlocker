# NextDnsBetBlocker Importer ğŸ“¥

## VisÃ£o Geral

O **Importer** Ã© um serviÃ§o console .NET que executa **uma Ãºnica vez por dia** em Azure Container Instances (ACI). Sua responsabilidade Ã©:

1. **Buscar listas de domÃ­nios** de fonte pÃºblica (Hagezi)
2. **Validar e deduplicar** os domÃ­nios
3. **Particionar e batch** para otimizar I/O no Table Storage
4. **Armazenar com histÃ³rico** em Table Storage para acesso posterior
5. **Prover checkpoints** para recuperaÃ§Ã£o de falhas

---

## Responsabilidades

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        NextDnsBetBlocker Importer Pipeline          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  1ï¸âƒ£  Fetch Lists (Hagezi)                           â”‚
â”‚      â”œâ”€ Download HTTP                               â”‚
â”‚      â””â”€ ValidaÃ§Ã£o de formato                        â”‚
â”‚                                                      â”‚
â”‚  2ï¸âƒ£  Parse & Validate                               â”‚
â”‚      â”œâ”€ Parsing de domÃ­nios                         â”‚
â”‚      â”œâ”€ Deduplicate in-memory                       â”‚
â”‚      â””â”€ Log invalids                                â”‚
â”‚                                                      â”‚
â”‚  3ï¸âƒ£  Partition by Hash                              â”‚
â”‚      â”œâ”€ EstratÃ©gia: hash(domain) % N partiÃ§Ãµes     â”‚
â”‚      â””â”€ DistribuiÃ§Ã£o uniforme                       â”‚
â”‚                                                      â”‚
â”‚  4ï¸âƒ£  Parallel Batch Processing                      â”‚
â”‚      â”œâ”€ Adaptive parallelism (5-30 tasks)          â”‚
â”‚      â”œâ”€ Batch size: 100 items                       â”‚
â”‚      â”œâ”€ Rate limiting: 2k ops/s per partition      â”‚
â”‚      â””â”€ Exponential backoff on timeout             â”‚
â”‚                                                      â”‚
â”‚  5ï¸âƒ£  Insert to Table Storage                        â”‚
â”‚      â”œâ”€ PartitionKey: list_name + hash_partition   â”‚
â”‚      â””â”€ RowKey: domain                              â”‚
â”‚                                                      â”‚
â”‚  6ï¸âƒ£  Update Checkpoint                              â”‚
â”‚      â”œâ”€ Mark list as imported                       â”‚
â”‚      â””â”€ Record timestamp                            â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Arquitetura da Pipeline

### Fluxo Sequencial: Hagezi â†’ ValidaÃ§Ã£o â†’ Armazenamento

```
Input (Hagezi Lists)
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HttpDownloadService     â”‚  â—„â”€â”€ Throttling externo
â”‚  - Download lists        â”‚      (respeita 429 Rate Limits)
â”‚  - Timeout: 30s          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GenericListImporter     â”‚
â”‚  - Parse domains         â”‚
â”‚  - Validate format       â”‚
â”‚  - Deduplicate           â”‚
â”‚  - Log metrics           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PartitionKeyStrategy     â”‚  â—„â”€â”€ EstratÃ©gia: MD5(domain) % N_partitions
â”‚ - Hash partitioning      â”‚      (garante distribuiÃ§Ã£o uniforme)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ParallelBatchManager                 â”‚  â—„â”€â”€ Core da otimizaÃ§Ã£o
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”‚ PartiÃ§Ã£o 00                        â”‚
â”‚ â”‚  Channel (batch queue)             â”‚
â”‚ â”‚  â”œâ”€ RateLimiter (2k ops/s)        â”‚
â”‚ â”‚  â”œâ”€ SemaphoreSlim (concurrency)   â”‚
â”‚ â”‚  â””â”€ Exponential backoff           â”‚
â”‚ â”‚                                    â”‚
â”‚ â”‚ Consumer Task (batch processing)   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”‚ PartiÃ§Ã£o 01, 02, ..., N            â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                      â”‚
â”‚  Global RateLimiter (20k ops/s)
â”‚  Global SemaphoreSlim (30 HTTP tasks)
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ListTableStorageRepositoryâ”‚
â”‚ - Batch insert           â”‚
â”‚ - Partition strategy     â”‚
â”‚ - Error handling         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Azure Table Storage      â”‚
â”‚ - Hagezi_Blocklists      â”‚
â”‚ - Tranco_TopDomains      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CheckpointStore          â”‚
â”‚ - Mark list as imported  â”‚
â”‚ - Record timestamp       â”‚
â”‚ - Enable recovery        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Design Patterns & TÃ©cnicas

### 1. **Adaptive Parallelism** ğŸ¯

**Problema**: NÃ£o sabemos quantas tarefas paralelas a rede/API suporta  
**SoluÃ§Ã£o**: ComeÃ§ar com um grau inicial e reduzir 5% a cada timeout

```
Monitoramento em tempo real:
- RecordTimeout() â†’ Reduz grau em 5%
- RecordSuccess() â†’ MantÃ©m atual
- GetStats() â†’ Retorna (timeouts, successes, current, initial)

Exemplo:
Initial = 30
â”œâ”€ Sucesso x10 â†’ mantÃ©m 30
â”œâ”€ Timeout    â†’ reduz para 29 (30 * 0.95)
â”œâ”€ Timeout    â†’ reduz para 27 (29 * 0.95)
â””â”€ MÃ­n = 5 tasks (floor garantido)
```

**BenefÃ­cio**: RecuperaÃ§Ã£o automÃ¡tica sob sobrecarga sem configuraÃ§Ã£o manual

### 2. **Hierarchical Throttling** ğŸš¦

MÃºltiplas camadas de rate limiting para evitar saturaÃ§Ã£o:

```
Per-Partition Rate Limiter (2k ops/s)
        â†“
    Per-Partition SemaphoreSlim (concurrency limit)
        â†“
    Global Rate Limiter (20k ops/s)
        â†“
    Global SemaphoreSlim (30 HTTP tasks mÃ¡ximo)
        â†“
    External API (Hagezi/HTTP)
```

**ImplementaÃ§Ã£o**:
- `ImportRateLimiter`: Sliding window (1s) com Queue<timestamp>
- `SemaphoreSlim`: Limite fÃ­sico de concorrÃªncia
- Backpressure: Channels bounded causam WaitAsync se cheios
- **âš™ï¸ [RECENTE FIX]**: Burst rate agora sincronizado corretamente com effective rate. Ver [THROTTLING_IMPROVEMENTS.md](THROTTLING_IMPROVEMENTS.md) para detalhes

### 3. **Graceful Degradation - Por PartiÃ§Ã£o** âš¡

Falha isolada de uma partiÃ§Ã£o nÃ£o afeta outras:

```
PartiÃ§Ã£o_00: Timeout
  â”œâ”€ aplica exponential backoff (100ms â†’ 200ms â†’ 400ms...)
  â”œâ”€ tenta retry (mÃ¡x 3x)
  â””â”€ se falhar: log + continue (nÃ£o bloqueia)

PartiÃ§Ã£o_01, 02, ..., N:
  â””â”€ continuam processando normalmente
```

**Resultado**: Throughput degradado mas nÃ£o zero

### 4. **Distributed Batch Isolation** ğŸ“¦

Cada partiÃ§Ã£o tem seu prÃ³prio:
- **Channel**: Producer/Consumer desacoplado
- **RateLimiter**: Controle independente de throughput
- **SemaphoreSlim**: Limite de concorrÃªncia isolado
- **Backoff**: Retry strategy prÃ³pria

```
ParallelBatchManager
â”œâ”€ PartitionConsumer (partition_00)
â”‚  â”œâ”€ Channel (100 items max)
â”‚  â”œâ”€ RateLimiter (2k ops/s)
â”‚  â”œâ”€ SemaphoreSlim (concurrency)
â”‚  â””â”€ CurrentBackoff (retry strategy)
â”œâ”€ PartitionConsumer (partition_01)
â”‚  â””â”€ [idem]
â””â”€ Global: RateLimiter (20k) + SemaphoreSlim (30)
```

**Exemplo**:
- partition_00 sofre timeout â†’ backoff = 400ms
- partition_01 continua com backoff = 0 (normal)

### 5. **Producer-Consumer com Pipelining** ğŸ”„

Desacoplamento entre produÃ§Ã£o de batches e consumo (processamento HTTP):

```
Main Thread (Producer)
  â”œâ”€ Parse domains (CPU-bound)
  â”œâ”€ Form batches (100 items)
  â””â”€ Enqueue to Channel (async, non-blocking)
       â”‚
       â–¼ Channel (bounded)
       â”‚
  Consumer Tasks (per partition)
  â”œâ”€ Wait for rate limit
  â”œâ”€ Acquire semaphore slot
  â”œâ”€ HTTP POST to Table Storage (I/O)
  â”œâ”€ Release semaphore
  â””â”€ Repeat
```

**BenefÃ­cio**: CPU e I/O nÃ£o se bloqueiam; mÃ¡xima eficiÃªncia

---

## ConfiguraÃ§Ã£o

### appsettings.json

```json
{
  "ListImportConfig": [
    {
      "ListName": "Hagezi_Blocklists",
      "SourceUrl": "https://raw.githubusercontent.com/hagezi/dns-blocklists/main/adblock/fake-phishing-awareness.txt",
      "Description": "Hagezi Fake Phishing Awareness List"
    },
    {
      "ListName": "Tranco_TopDomains",
      "SourceUrl": "https://tranco-list.eu/top-4.8m.csv.zip",
      "Description": "Tranco Top 4.8M Legitimate Domains"
    }
  ],
  "ParallelImportConfig": {
    "InitialDegreeOfParallelism": 20,
    "BatchSize": 100,
    "ChannelCapacityPerPartition": 100,
    "MaxConcurrencyPerPartition": 5,
    "MaxOperationsPerSecondPerPartition": 2000,
    "MaxGlobalOperationsPerSecond": 20000,
    "MaxGlobalConcurrentRequests": 30,
    "PartitionCount": 32,
    "MaxRetries": 3
  }
}
```

### Tuning para Different Environments

| Ambiente | InitialParallelism | PartitionCount | BatchSize | MaxOpsPerSecond | Rationale |
|----------|------------------|-----------------|-----------|-----------------|-----------|
| Local Dev | 5 | 8 | 100 | 2k (global) | Evita saturaÃ§Ã£o de conexÃµes |
| Staging | 15 | 16 | 100 | 10k (global) | Teste antes de produÃ§Ã£o |
| Production | 25-30 | 32 | 100 | 20k (global) | MÃ¡xima throughput com safety |
| **Optimized Prod** | 25-30 | 32 | **500** | 20k (global) | **80% cost reduction** (future) |

**Nota**: Recommended upgrade path:
1. Deploy with BatchSize=100 (safe, proven)
2. Monitor for 1-2 weeks
3. Upgrade to BatchSize=500 if no 429 errors
4. Monitor for 1-2 weeks
5. Tune parallelism based on observed throughput

**Scaling Guidelines for Larger Datasets**:
```
< 1M items:     InitialParallelism=10, PartitionCount=8
1M - 5M items:  InitialParallelism=20, PartitionCount=16-32  â† CURRENT
5M - 50M items: InitialParallelism=30, PartitionCount=64
> 50M items:    Consider: Multi-table strategy or Premium tier
```

---

## Performance Characteristics

### Throughput Esperado

```
CenÃ¡rio: 5M domÃ­nios (Tranco 4.8M + Hagezi Blocklists)

Com Adaptive Parallelism:
â”œâ”€ Batch size: 100
â”œâ”€ Rate limit: 2k ops/s per partition
â”œâ”€ Partitions: 32
â”œâ”€ Effective throughput: 32 * 2k = ~64k ops/s (teÃ³rico)
â”œâ”€ Real (com latÃªncia HTTP 200ms): ~10k-15k ops/s
â””â”€ Time to import: ~350-500s (~6-8 minutos)

vs. Sequential (1 thread):
â”œâ”€ Rate limit: 2k ops/s
â”œâ”€ HTTP latency: 200ms (5 parallel batches mÃ¡ximo)
â””â”€ Time to import: ~2500-3000s (40-50 minutos, 5-7x mais lento)

Note: Tranco 4.8M Ã© 4.8x maior que 1M, mas throughput escalÃ¡vel mantÃ©m tempo linear
```

### Memory Footprint

```
Channel buffer: 32 partitions Ã— 100 items = 3.2k items
Per item: ~100 bytes (domain string + metadata)
Total: ~320 KB (negligÃ­vel)

Deduplication (in-memory): ~240MB (para 4.8M domÃ­nios com HashSet<string>)
Overall: <400 MB para operaÃ§Ã£o completa com Tranco full + Hagezi lists
```

### CPU Utilization

```
Producer (parsing): 1 core (~30% utilization)
Consumers (I/O wait): 8 cores (~5% utilization)
- Maioria do tempo esperando HTTP response (I/O bound)
- Minimal CPU contention
```

---

## Azure Table Storage Optimization & Quota Management ğŸ¯

**ğŸ“– Para guia operacional detalhado, veja [TABLE_STORAGE_OPERATIONAL_GUIDE.md](TABLE_STORAGE_OPERATIONAL_GUIDE.md)**

Nesta seÃ§Ã£o documentamos estratÃ©gias arquiteturais. Para troubleshooting em tempo real e checklists operacionais, consulte o guia acima.

### Contexto: Operando PrÃ³ximo aos Limites MÃ¡ximos

Com 5M+ domÃ­nios importados, o Table Storage requer **estratÃ©gia cuidadosa** para evitar throttling (429) e maximizar throughput:

```
Azure Table Storage Limits (por partition key):
â”œâ”€ Throughput: 20,000 RUs/s (eventual consistency)
â”œâ”€ Entity size: 1 MB mÃ¡ximo
â”œâ”€ Batch size: 100 entidades mÃ¡ximo
â”œâ”€ Request rate: ~20k requests/s por partition
â””â”€ Concurrent connections: Limited by storage account
```

### EstratÃ©gia de Particionamento para Escalabilidade

A escolha do **PartitionKey** Ã© crÃ­tica para evitar hot-spots:

```
Nossa estratÃ©gia: hash(domain) % N_partitions

Exemplo: 32 partiÃ§Ãµes
â”œâ”€ example.com â†’ MD5 hash â†’ 0x3f... â†’ 0x3f % 32 = partition_15
â”œâ”€ google.com  â†’ MD5 hash â†’ 0x7a... â†’ 0x7a % 32 = partition_26
â””â”€ amazon.com  â†’ MD5 hash â†’ 0x1b... â†’ 0x1b % 32 = partition_27

DistribuiÃ§Ã£o uniforme:
â”œâ”€ Sem hot-spots (cada partiÃ§Ã£o recebe ~156k domÃ­nios = 5M/32)
â”œâ”€ Throughput balanceado entre partiÃ§Ãµes
â””â”€ EscalÃ¡vel: adicione partiÃ§Ãµes se N aumentar
```

### CÃ¡lculo de Throughput Consumido

```
Throughput Storage Unit (TU) = (Data written / 1 KB) + (Operations / 100)

CenÃ¡rio: 5M domÃ­nios, 1x/dia

Dados por entidade:
â”œâ”€ PartitionKey: ~20 bytes (e.g., "Tranco_4")
â”œâ”€ RowKey: ~50 bytes (domain name)
â”œâ”€ Timestamp: 8 bytes (automÃ¡tico)
â”œâ”€ properties: category, source, timestamp_imported (~100 bytes)
â””â”€ Total por entidade: ~178 bytes

CÃ¡lculo diÃ¡rio:
â”œâ”€ Write operations: 5,000,000 inserts
â”œâ”€ Data written: 5M * 178 bytes = ~890 MB = ~890,000 KB
â”œâ”€ TU = (890,000 / 1) + (5,000,000 / 100) = 890,000 + 50,000 = 940,000 TUs/dia
â”œâ”€ Spread over 7 minutos: 940,000 / 0.116 hrs = ~8.1M RUs/s
â””â”€ Peak: ~8.1M RUs/s >> Table limit (20k RUs/s) âŒ THROTTLE!

SEM rate limiting (problema):
â””â”€ Servidor rejeitarÃ¡ com 429 Too Many Requests

COM rate limiting hierÃ¡rquico (soluÃ§Ã£o):
â”œâ”€ Per-partition: 2k ops/s â†’ 6.4k ops/s (32 partitions)
â”œâ”€ Global: 20k ops/s â†’ 20k ops/s (Table limit)
â”œâ”€ Spread time: 5M / 20k = 250s = ~4 minutos âœ“
â””â”€ Throughput consumido: Respeitado ao mÃ¡ximo
```

### Batch Sizing: Trade-off Throughput vs Latency

```
Batch Size Analysis (100 itens por batch):

Com 100 items/batch:
â”œâ”€ Batches necessÃ¡rios: 5M / 100 = 50,000 batches
â”œâ”€ Batch request overhead: ~200 bytes
â”œâ”€ Total overhead: 50k * 200 = 10 MB
â”œâ”€ LatÃªncia por batch: ~50ms (HTTP RTT)
â”œâ”€ Total time: 50k batches * 50ms = 2500s (SEQUENTIAL) âŒ

Parallelizado (32 partitions, 20 concurrent):
â”œâ”€ Batches por partiÃ§Ã£o: 50k / 32 = 1,562 batches
â”œâ”€ Concurrent batches: min(1562, 20 concurrent limit) = 20
â”œâ”€ Throughput: 20 * 100 items / 50ms = 40k items/s
â”œâ”€ Time: 5M / 40k = 125s âœ“
â””â”€ Eficiente!

vs. Batch Size = 1000 (maior):
â”œâ”€ Batches: 5,000 (5x menor)
â”œâ”€ LatÃªncia por batch: ~100ms (network overhead)
â”œâ”€ Concurrent: 20
â”œâ”€ Throughput: 20 * 1000 items / 100ms = 200k items/s
â”œâ”€ Time: 5M / 200k = 25s âœ“âœ“ (mais rÃ¡pido!)
â””â”€ Trade-off: mais memÃ³ria (~40MB vs ~4MB)

RecomendaÃ§Ã£o: 100 Ã© bom balanÃ§o para production (seguranÃ§a + performance)
```

### Monitoramento de Quota em Tempo Real

```kusto
// Application Insights query para monitorar consumo de quota

// 1. Taxas de requisiÃ§Ã£o (para alertar se prÃ³ximo a 20k/s)
customMetrics
| where name == "TableStorageRequests.Count"
| summarize requests_per_sec = count() / (max(timestamp) - min(timestamp)) 
           by tostring(customDimensions.PartitionKey)
| where requests_per_sec > 15000  // 75% do limite

// 2. Taxas de erro 429 (throttle)
customMetrics
| where name == "TableStorageError429.Count"
| summarize throttle_count = sum(value) by bin(timestamp, 1m)
| where throttle_count > 0

// 3. LatÃªncia de operaÃ§Ã£o (indicador de contention)
customMetrics
| where name == "TableStorageLatency.Milliseconds"
| summarize avg_latency = avg(value), p99_latency = percentile(value, 99)
           by bin(timestamp, 5m)

// 4. Data written per partition
customMetrics
| where name == "TableStorageData.BytesWritten"
| summarize total_bytes = sum(value)
           by tostring(customDimensions.PartitionKey)
```

### Alertas Recomendados

| MÃ©trica | Threshold | AÃ§Ã£o |
|---------|-----------|------|
| **429 Errors** | > 5/min | âš ï¸ Reduzir `MaxGlobalOperationsPerSecond` (20k â†’ 15k) |
| **Request Rate** | > 18k/s | âš ï¸ Aumentar `MaxRetries` (backoff mais agressivo) |
| **Partition Hot-spot** | Variance > 30% | âš ï¸ Revisar estratÃ©gia de particionamento (hash distribution) |
| **Latency P99** | > 1000ms | âš ï¸ Verificar capacity (scale up, RU aumentadas) |
| **Storage Size** | > 90% quota | âš ï¸ Planejar archival/retention (deletar dados antigos) |

### OtimizaÃ§Ãµes AvanÃ§adas

#### 1. **Reuse Table Rows (Update em vez de Insert)**
```
Se Tranco muda incrementalmente (removals < 1%):
â”œâ”€ V1 (Insert only): 5M inserts
â”œâ”€ V2 (Update existing): 50k updates + 5M inserts = 5.05M ops (1% economia)
â””â”€ BenefÃ­cio: pequeno neste caso, mas considerÃ¡vel em updates parciais
```

#### 2. **Batch Deletes de Dados Obsoletos**
```
RetenÃ§Ã£o de 90 dias:
â”œâ”€ Cron: Daily @ 03:00 UTC
â”œâ”€ Delete: domains com Timestamp < (now - 90 days)
â”œâ”€ Batch delete: atÃ© 100 entities
â””â”€ Throughput: similar ao import (20k ops/s limite)

Exemplo:
  dia 1 (5M insert) â†’ 5M storage
  dia 2 (5M insert, 0 delete) â†’ 10M storage
  ...
  dia 90 (5M insert, 0 delete) â†’ 450M storage
  dia 91 (5M insert, 5M delete) â†’ 450M storage (steady state)
```

#### 3. **Compression em RowKey para Reduzir I/O**
```
Atual: RowKey = "example.com" (~11 bytes)
Otimizado: RowKey = base36(hash) (~8 bytes)
â””â”€ Economia: 5M * 3 bytes = 15 MB (negligÃ­vel)

NÃ£o recomendado: readability loss > benefÃ­cio
```

### Storage Account Quotas

```
Standard Storage Account (default):
â”œâ”€ Max capacity: 500 TB (per account)
â”œâ”€ Max requests: 20k RUs/s (per partition)
â”œâ”€ Max entities: Unlimited
â””â”€ Cost: Pay-as-you-go (data + transactions)

Com 5M * 178 bytes = ~890 MB:
â”œâ”€ Storage cost: ~$0.02/mÃªs (negligÃ­vel)
â”œâ”€ Transaction cost: ~$0.50/mÃªs (50M ops @ $0.01 per 10k ops)
â””â”€ Total: ~$0.52/mÃªs (muito barato!)

Crescimento (10 anos, 5M/dia):
â”œâ”€ Storage: 5M * 365 * 10 * 178 bytes = 3.25 TB (< 500 TB limit âœ“)
â”œâ”€ Archival: Delete apÃ³s 90 dias (rolling window)
â””â”€ Cost: ContÃ­nuo ~$0.50/mÃªs
```

---

## Monitoramento & Observabilidade

### Logs Estruturados

```
[2024-01-15 02:00:00] INF Import Pipeline started (Schedule: Daily @ 02:00 UTC)
[2024-01-15 02:00:15] INF Tranco_TopDomains: Downloaded 4,800,000 items (78 MB)
[2024-01-15 02:00:30] INF Tranco_TopDomains: Validated 4,750,000 items (1.0% duplicates)
[2024-01-15 02:00:35] INF Hagezi_Blocklists: Downloaded 250,000 items
[2024-01-15 02:00:40] INF Hagezi_Blocklists: Validated 248,500 items (0.6% duplicates)
[2024-01-15 02:00:45] INF [Adaptive] Initial parallelism: 25, Partitions: 32
[2024-01-15 02:00:50] INF Partition distribution: ~153k items/partition (balanced)
[2024-01-15 02:01:00] DBG Batch 1: 100 items â†’ partition_00 â†’ 5ms
[2024-01-15 02:01:05] DBG Batch 2: 100 items â†’ partition_01 â†’ 5ms
[2024-01-15 02:01:10] WRN [Adaptive] âš  Timeout on partition_05! Reducing parallelism: 25 â†’ 24
[2024-01-15 02:02:00] INF [Throttle] Rate: 18.5k ops/s (approaching 20k limit)
[2024-01-15 02:04:30] INF âœ“ All partitions processed: 4.998M items in 270s (~18.5k items/s)
[2024-01-15 02:04:35] INF [Quota] Storage consumed: ~890 MB, Requests: ~5M, Cost: ~$0.50
[2024-01-15 02:04:40] INF Checkpoint recorded: Tranco @ 2024-01-15 02:04:30, Hagezi @ 2024-01-15 02:04:35
[2024-01-15 02:04:45] INF âœ“ Import Pipeline completed successfully
```

### Application Insights Queries

```kusto
// MÃ©trica: Taxa de processamento por segundo (deve estar prÃ³ximo a 20k ops/s limite)
customMetrics
| where name == "ImportMetrics.ItemsProcessed"
| summarize count=sum(value) by bin(timestamp, 10s)
| extend ops_per_sec = count / 10

// Erros 429 (throttle) - alertar se > 0
customMetrics
| where name == "ImportMetrics.Error429Count"
| summarize errors=sum(value) by tostring(customDimensions.PartitionKey)

// LatÃªncia de operaÃ§Ã£o por partiÃ§Ã£o
customMetrics
| where name == "ImportMetrics.PartitionLatency"
| summarize avg_ms=avg(value), p99_ms=percentile(value, 99)
           by tostring(customDimensions.PartitionKey)

// DistribuiÃ§Ã£o de carga (verificar se uniforme)
customMetrics
| where name == "ImportMetrics.PartitionItemCount"
| summarize items=sum(value) by tostring(customDimensions.PartitionKey)
| summarize avg_items=avg(items), variance=stdev(items)

// Consumo de quota (storage bytes + transactions)
customMetrics
| where name == "ImportMetrics.StorageConsumed"
| summarize total_mb=sum(value) by bin(timestamp, 1h)
```

### Health Check Endpoints

Embora o Importer seja one-shot, registra seu status:
```json
{
  "status": "completed",
  "duration_seconds": 270,
  "items_processed": 4998500,
  "items_failed": 1500,
  "success_rate": 0.9997,
  "final_parallelism": 24,
  "partition_distribution": {
    "min_items": 150000,
    "max_items": 157000,
    "variance": "4.2%"
  },
  "table_storage": {
    "storage_consumed_mb": 890,
    "requests": 50000,
    "cost_estimate": 0.50,
    "throttle_events": 0
  }
}
```

---

## Tratamento de Erros & Recovery

### EstratÃ©gias por Tipo de Erro

| Erro | EstratÃ©gia | Exemplo |
|------|-----------|---------|
| **Timeout (> 30s)** | Exponential backoff per partition | 100ms â†’ 200ms â†’ 400ms... |
| **429 (Rate Limit)** | Global slowdown + per-partition reduction | Reduce all partitions by 5% |
| **404 (Lista nÃ£o existe)** | Skip + Log | Continue to next list |
| **Partial batch failure** | Retry individual items | 3 tentativas mÃ¡ximo |
| **Checkpoint error** | Log + Continue | Progress saved in next run |

### Circuit Breaker ImplÃ­cito

```
consecutiveTimeouts > 10:
  â”œâ”€ Log warning
  â”œâ”€ Reduce parallelism to minimum (5)
  â””â”€ Exponential backoff atÃ© 5s
```

---

## Deployment

### Local Development

```bash
# Via Docker
docker build -t nextdnsblocker-importer:latest .
docker run --rm \
  -e "AzureWebJobsStorage=DefaultEndpointsProtocol=https;..." \
  -e "ASPNETCORE_ENVIRONMENT=Development" \
  nextdnsblocker-importer:latest

# Via dotnet CLI
dotnet run --project src/NextDnsBetBlocker.Worker.Importer
```

### Azure Container Instances (Production)

```bash
az container create \
  --resource-group rg-dnsblocker \
  --name importer-daily \
  --image acr.azurecr.io/nextdnsblocker-importer:latest \
  --cpu 2 \
  --memory 1 \
  --restart-policy Never
```

### Agendamento (Azure Container Apps / Logic Apps)

```yaml
Schedule: Daily @ 02:00 UTC
â”œâ”€ Baixa carga de rede
â”œâ”€ Fora de horÃ¡rio de pico dos workers
â””â”€ Margem antes de Worker consumir dados (06:00)
```

---

## Trade-offs & DecisÃµes de Design

### âœ… Por que Particionamento?

**Alternativa**: Uma fila Ãºnica para todos os domÃ­nios
- âœ— Contention em lock (um channel, muitos threads)
- âœ— Backoff global (falha em 1 partiÃ§Ã£o afeta todas)
- âœ— MemÃ³ria: canal Ãºnico com 5M items

**Nossa abordagem**: 32 partiÃ§Ãµes independentes (hash-based)
- âœ“ Isolamento: falha local nÃ£o afeta globais
- âœ“ Escalabilidade: menos contention
- âœ“ MemÃ³ria: distribuÃ­do (5M / 32 = ~156k items/partition)
- âœ“ Table Storage: evita hot-spots, distribui carga uniformemente

### âœ… Por que Batch Size = 100?

**Alternativa 1**: Batch size = 1 (insert individual)
- âœ— 50M HTTP requests (vs 500k com batch)
- âœ— Overhead de connection setup (5-10x mais lento)
- âœ— Custo: 10x mais ops contabilizadas

**Alternativa 2**: Batch size = 1000
- âœ“ 5k requests (vs 500k)
- âœ— MemÃ³ria: 1k items * 32 partitions = 32k em buffer
- âœ— LatÃªncia: ~100ms por batch (vs ~50ms com 100)
- âœ— Network: possÃ­vel timeout se item lento

**Nossa abordagem**: Batch size = 100
- âœ“ 500k requests (bom balanÃ§o)
- âœ“ MemÃ³ria: 100 * 32 = 3.2k items (negligÃ­vel)
- âœ“ LatÃªncia: ~50ms por batch
- âœ“ ResiliÃªncia: retry mais granular

### âœ… Por que Adaptive Parallelism?

**Alternativa**: Fixed parallelism (ex: sempre 25)
- âœ— Muito agressivo em rede lenta â†’ timeouts
- âœ— Muito conservador â†’ subutilizaÃ§Ã£o de recursos
- âœ— Sem auto-recovery quando pressÃ£o reduz

**Nossa abordagem**: ComeÃ§a alto, reduz 5% em timeout
- âœ“ Auto-tuning sem configuraÃ§Ã£o manual
- âœ“ RecuperaÃ§Ã£o automÃ¡tica quando pressÃ£o reduz
- âœ“ AdaptÃ¡vel a diferentes ambientes (local, ACI, cloud)

### âœ… Por que Producer-Consumer?

**Alternativa**: Processamento sÃ­ncrono (parse + store em sÃ©rie)
- âœ— CPU bloqueia esperando I/O (HTTP latency 50-200ms)
- âœ— Throughput reduzido 5-10x
- âœ— Sem pipelining

**Nossa abordagem**: Canais desacoplam produtor (parse) de consumidor (HTTP)
- âœ“ MÃ¡xima utilizaÃ§Ã£o de I/O (pipelining)
- âœ“ CPU nunca bloqueia no I/O
- âœ“ Backpressure automÃ¡tica (Channel bounded)

### âœ… Por que Rate Limiting HierÃ¡rquico?

**Alternativa 1**: Sem rate limiting (agressivo)
- âœ— Gera 429 erros do Table Storage
- âœ— Retry exponencial â†’ execuÃ§Ã£o 2-3x mais lenta
- âœ— Quota exceeded (bad reputation)

**Alternativa 2**: Rate limit Ãºnico (20k ops/s global)
- âœ— Falha em 1 partiÃ§Ã£o afeta todas (throttle global)
- âœ— Menos flexÃ­vel

**Nossa abordagem**: Per-partition (2k) + Global (20k)
- âœ“ Isolamento: falha local nÃ£o afeta globais
- âœ“ Respeita limites do Table Storage
- âœ“ Throughput mÃ¡ximo (prÃ³ximo ao limite)
- âœ“ RecuperaÃ§Ã£o graceful

---

## ConclusÃ£o

O Importer demonstra **padrÃµes enterprise-grade** para bulk data import em ambientes cloud:

- **Robustez**: Graceful degradation por partiÃ§Ã£o, checkpointing, retry automÃ¡tico
- **Performance**: Adaptive parallelism + rate limiting hierÃ¡rquico + pipelining
- **Escalabilidade**: Arquitetura agnÃ³stica ao volume (1M, 5M, 100M domÃ­nios)
- **Observabilidade**: Logging estruturado, Application Insights, quota monitoring
- **EficiÃªncia**: OperaÃ§Ã£o prÃ³xima ao mÃ¡ximo de throughput do Table Storage (20k ops/s)

Perfeito para um **portfÃ³lio tÃ©cnico** que demonstra domÃ­nio de:
- PadrÃµes de distribuiÃ§Ã£o (hash-based partitioning)
- ConcorrÃªncia (Channels, SemaphoreSlim, async/await)
- Cloud economics (quota management, cost optimization)
- Resilience engineering (graceful degradation, adaptive patterns)
// DistribuiÃ§Ã£o de carga (verificar se uniforme)
customMetrics
| where name == "ImportMetrics.PartitionItemCount"
| summarize items=sum(value) by tostring(customDimensions.PartitionKey)
| summarize avg_items=avg(items), variance=stdev(items)

// Consumo de quota (storage bytes + transactions)
customMetrics
| where name == "ImportMetrics.StorageConsumed"
| summarize total_mb=sum(value) by bin(timestamp, 1h)
```

### Health Check Endpoints

Embora o Importer seja one-shot, registra seu status:
```json
{
  "status": "completed",
  "duration_seconds": 95,
  "items_processed": 248500,
  "items_failed": 50,
  "success_rate": 0.998,
  "final_parallelism": 18
}
```

---

## Tratamento de Erros & Recovery

### EstratÃ©gias por Tipo de Erro

| Erro | EstratÃ©gia | Exemplo |
|------|-----------|---------|
| **Timeout (> 30s)** | Exponential backoff per partition | 100ms â†’ 200ms â†’ 400ms... |
| **429 (Rate Limit)** | Global slowdown + per-partition reduction | Reduce all partitions by 5% |
| **404 (Lista nÃ£o existe)** | Skip + Log | Continue to next list |
| **Partial batch failure** | Retry individual items | 3 tentativas mÃ¡ximo |
| **Checkpoint error** | Log + Continue | Progress saved in next run |

### Circuit Breaker ImplÃ­cito

```
consecutiveTimeouts > 10:
  â”œâ”€ Log warning
  â”œâ”€ Reduce parallelism to minimum (5)
  â””â”€ Exponential backoff atÃ© 5s
```

---

## Deployment

### Local Development

```bash
# Via Docker
docker build -t nextdnsblocker-importer:latest .
docker run --rm \
  -e "AzureWebJobsStorage=DefaultEndpointsProtocol=https;..." \
  -e "ASPNETCORE_ENVIRONMENT=Development" \
  nextdnsblocker-importer:latest

# Via dotnet CLI
dotnet run --project src/NextDnsBetBlocker.Worker.Importer
```

### Azure Container Instances (Production)

```bash
az container create \
  --resource-group rg-dnsblocker \
  --name importer-daily \
  --image acr.azurecr.io/nextdnsblocker-importer:latest \
  --cpu 2 \
  --memory 1 \
  --restart-policy Never
```

### Agendamento (Azure Container Apps / Logic Apps)

```yaml
Schedule: Daily @ 02:00 UTC
â”œâ”€ Baixa carga de rede
â”œâ”€ Fora de horÃ¡rio de pico dos workers
â””â”€ Margem antes de Worker consumir dados (06:00)
```

---

## Trade-offs & DecisÃµes de Design

### âœ… Por que Particionamento?

**Alternativa**: Uma fila Ãºnica para todos os domÃ­nios
- âœ— Contention em lock (um channel, muitos threads)
- âœ— Backoff global (falha em 1 partiÃ§Ã£o afeta todas)
- âœ— MemÃ³ria: canal Ãºnico com 1M items

**Nossa abordagem**: 32 partiÃ§Ãµes independentes
- âœ“ Isolamento: falha local nÃ£o afeta globais
- âœ“ Escalabilidade: menos contention
- âœ“ MemÃ³ria: distribuÃ­do (32 Ã— 3.2k items)

### âœ… Por que Adaptive Parallelism?

**Alternativa**: Fixed parallelism (ex: sempre 20)
- âœ— Muito agressivo â†’ timeouts em rede lenta
- âœ— Muito conservador â†’ subutilizaÃ§Ã£o de recursos

**Nossa abordagem**: ComeÃ§a alto, reduz sob pressÃ£o
- âœ“ Auto-tuning sem configuraÃ§Ã£o
- âœ“ RecuperaÃ§Ã£o automÃ¡tica quando pressÃ£o reduz

### âœ… Por que Producer-Consumer?

**Alternativa**: Processamento sÃ­ncrono (parse + store em sÃ©rie)
- âœ— CPU bloqueia esperando I/O
- âœ— Throughput reduzido 5-10x

**Nossa abordagem**: Canais desacoplam produtor (parse) de consumidor (HTTP)
- âœ“ MÃ¡xima utilizaÃ§Ã£o de I/O
- âœ“ Pipelining natural

---

## ConclusÃ£o

O Importer demonstra **padrÃµes enterprise-grade** para bulk data import em ambientes cloud:

- **Robustez**: Graceful degradation por partiÃ§Ã£o
- **Performance**: Adaptive parallelism + rate limiting hierÃ¡rquico
- **Observabilidade**: Logging estruturado e mÃ©tricas
- **Escalabilidade**: Arquitetura agnÃ³stica ao volume

Perfeito para um **portfÃ³lio tÃ©cnico** que demonstra domÃ­nio de padrÃµes modernos de engenharia!
